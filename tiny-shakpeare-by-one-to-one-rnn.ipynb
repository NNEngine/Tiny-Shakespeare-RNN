{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom datasets import Dataset\nimport requests","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:36.205890Z","iopub.execute_input":"2025-10-15T05:09:36.206286Z","iopub.status.idle":"2025-10-15T05:09:39.399291Z","shell.execute_reply.started":"2025-10-15T05:09:36.206253Z","shell.execute_reply":"2025-10-15T05:09:39.398265Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device.type)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:39.401773Z","iopub.execute_input":"2025-10-15T05:09:39.402303Z","iopub.status.idle":"2025-10-15T05:09:39.408930Z","shell.execute_reply.started":"2025-10-15T05:09:39.402275Z","shell.execute_reply":"2025-10-15T05:09:39.407674Z"}},"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\nresp = requests.get(url)\ntext = resp.text\nprint(\"Length of text:\", len(text))\nprint(\"\\n\\n\")\nprint(text[:500])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:39.410247Z","iopub.execute_input":"2025-10-15T05:09:39.410702Z","iopub.status.idle":"2025-10-15T05:09:39.577034Z","shell.execute_reply.started":"2025-10-15T05:09:39.410667Z","shell.execute_reply":"2025-10-15T05:09:39.575946Z"}},"outputs":[{"name":"stdout","text":"Length of text: 1115394\n\n\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"ds = Dataset.from_dict({\"text\" : [text]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:39.578004Z","iopub.execute_input":"2025-10-15T05:09:39.578284Z","iopub.status.idle":"2025-10-15T05:09:39.601796Z","shell.execute_reply.started":"2025-10-15T05:09:39.578263Z","shell.execute_reply":"2025-10-15T05:09:39.600632Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"chars = sorted(set(text))\nvocab_size = len(chars)\nstoi = {ch: i for i, ch in enumerate(chars)} # String to integer\nprint(stoi)\n\nprint(\"\\n\")\n\nitos = {i: ch for ch, i in stoi.items()} # intereg to string\nprint(itos)\n\ndef encode(s):\n    return [stoi[c] for c in s]\n\ndata = torch.tensor(encode(text), dtype=torch.long)\n\nprint(\"\\n\")\n\nprint(data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:39.603383Z","iopub.execute_input":"2025-10-15T05:09:39.603784Z","iopub.status.idle":"2025-10-15T05:09:39.796208Z","shell.execute_reply.started":"2025-10-15T05:09:39.603695Z","shell.execute_reply":"2025-10-15T05:09:39.795264Z"}},"outputs":[{"name":"stdout","text":"{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n\n\n{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n\n\ntensor([18, 47, 56,  ..., 45,  8,  0])\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"seq_len = 60\n\ndef get_batches(data, seq_len, batch_size=128):\n    n = len(data) - seq_len\n    ix = torch.randint(0, n, (batch_size,))\n    x = torch.stack([data[i : i+seq_len] for i in ix])\n    y = torch.stack([data[i+1 : i+seq_len+1] for i in ix])\n    return x, y\n\nclass CharRNN(nn.Module):\n    def __init__(self, vocab_size, hidden_size,num_layers=2):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.embed = nn.Embedding(vocab_size, hidden_size)\n        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=True)\n        self.fc = nn.Linear(hidden_size, vocab_size)\n\n    def forward(self, x, h=None):\n        x = self.embed(x)\n        out, h = self.rnn(x, h)\n        out = self.fc(out)\n        return out, h","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:39.797069Z","iopub.execute_input":"2025-10-15T05:09:39.797294Z","iopub.status.idle":"2025-10-15T05:09:39.805279Z","shell.execute_reply.started":"2025-10-15T05:09:39.797276Z","shell.execute_reply":"2025-10-15T05:09:39.804169Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CharRNN(vocab_size, hidden_size=128,num_layers=6).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:39.806696Z","iopub.execute_input":"2025-10-15T05:09:39.807333Z","iopub.status.idle":"2025-10-15T05:09:41.192237Z","shell.execute_reply.started":"2025-10-15T05:09:39.807308Z","shell.execute_reply":"2025-10-15T05:09:41.191199Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"epochs = 400\nfor epoch in range(epochs):\n    x_batch, y_batch = get_batches(data, seq_len)\n    x_batch = x_batch.to(device)\n    y_batch = y_batch.to(device)\n\n    optimizer.zero_grad()\n    out, _ = model(x_batch)\n    loss = criterion(out.view(-1, vocab_size), y_batch.view(-1))\n    loss.backward()\n    optimizer.step()\n\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch {epoch+1}/{epochs}, loss = {loss.item():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:09:41.194979Z","iopub.execute_input":"2025-10-15T05:09:41.195507Z","iopub.status.idle":"2025-10-15T05:10:00.395562Z","shell.execute_reply.started":"2025-10-15T05:09:41.195483Z","shell.execute_reply":"2025-10-15T05:10:00.394432Z"}},"outputs":[{"name":"stdout","text":"Epoch 10/400, loss = 3.5268\nEpoch 20/400, loss = 3.0135\nEpoch 30/400, loss = 2.7692\nEpoch 40/400, loss = 2.6672\nEpoch 50/400, loss = 2.5511\nEpoch 60/400, loss = 2.4754\nEpoch 70/400, loss = 2.4244\nEpoch 80/400, loss = 2.4074\nEpoch 90/400, loss = 2.3446\nEpoch 100/400, loss = 2.2842\nEpoch 110/400, loss = 2.2578\nEpoch 120/400, loss = 2.2146\nEpoch 130/400, loss = 2.2095\nEpoch 140/400, loss = 2.2090\nEpoch 150/400, loss = 2.2026\nEpoch 160/400, loss = 2.1430\nEpoch 170/400, loss = 2.1350\nEpoch 180/400, loss = 2.0967\nEpoch 190/400, loss = 2.0865\nEpoch 200/400, loss = 2.1199\nEpoch 210/400, loss = 2.0821\nEpoch 220/400, loss = 2.0780\nEpoch 230/400, loss = 2.0683\nEpoch 240/400, loss = 2.0454\nEpoch 250/400, loss = 2.0268\nEpoch 260/400, loss = 2.0070\nEpoch 270/400, loss = 2.0196\nEpoch 280/400, loss = 1.9949\nEpoch 290/400, loss = 1.9791\nEpoch 300/400, loss = 1.9985\nEpoch 310/400, loss = 1.9568\nEpoch 320/400, loss = 1.9941\nEpoch 330/400, loss = 1.9631\nEpoch 340/400, loss = 1.9365\nEpoch 350/400, loss = 1.9481\nEpoch 360/400, loss = 1.9509\nEpoch 370/400, loss = 1.9726\nEpoch 380/400, loss = 1.9162\nEpoch 390/400, loss = 1.9437\nEpoch 400/400, loss = 1.9099\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def generate_text(model, start_text=\"ROMEO\", length=100):\n    model.eval()\n    input_ids = torch.tensor([stoi[c] for c in start_text], dtype=torch.long).unsqueeze(0).to(device)\n    hidden = None\n    result = list(start_text)\n    for _ in range(length):\n        out, hidden = model(input_ids, hidden)\n        logits = out[0, -1]\n        next_id = torch.argmax(logits).item()\n        result.append(itos[next_id])\n        input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n    return ''.join(result)\n\nprint(\"Sample:\", generate_text(model, \"First Citizen\", 1000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-15T05:11:04.721642Z","iopub.execute_input":"2025-10-15T05:11:04.721955Z","iopub.status.idle":"2025-10-15T05:11:05.033460Z","shell.execute_reply.started":"2025-10-15T05:11:04.721932Z","shell.execute_reply":"2025-10-15T05:11:05.032059Z"}},"outputs":[{"name":"stdout","text":"Sample: First Citizen the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with the with \n","output_type":"stream"}],"execution_count":10}]}